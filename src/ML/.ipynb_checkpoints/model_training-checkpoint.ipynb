{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fda49cb0-8bb6-4baf-8cbb-63778218e2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c65e7376-f2f5-4c44-a7e5-31c1239136c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a standard CNN model. We will edit it later. \n",
    "\n",
    "def create_kanji_model():\n",
    "    model = models.Sequential([\n",
    "        layers.InputLayer(input_shape=(28, 28, 1)),  # Input shape for each image (28x28 grayscale)\n",
    "        \n",
    "        # First convolution layer with 32 filters and 3x3 kernel\n",
    "        layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),  # Max pooling to reduce spatial dimensions\n",
    "        \n",
    "        # Second convolution layer with 64 filters and 3x3 kernel\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),  # Max pooling again\n",
    "        \n",
    "        # Third convolution layer with 128 filters and 3x3 kernel\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),  # Max pooling again\n",
    "        \n",
    "        layers.Flatten(),  # Flatten the 3D output to 1D for fully connected layer\n",
    "        layers.Dense(128, activation='relu'),  # Fully connected layer with 128 units\n",
    "        layers.Dense(2, activation='softmax')  # Output layer for 10 kanji classes (softmax for multi-class classification)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da6f2fe1-721c-4e90-a776-7398687869ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = create_kanji_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ba91996-934e-4748-b093-821ab006f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '../ML/train'\n",
    "test_dir = '../ML/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f31bbad0-512c-40b3-bcdb-92f048140687",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bacb398-818a-4e5b-8eee-0a033673daa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 210 images belonging to 2 classes.\n",
      "Found 89 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator()\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(28, 28),  # Adjust image size if needed, only if necessary\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',  # Use categorical for multi-class classification\n",
    "    color_mode='grayscale',  # Use grayscale if your images are grayscale\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(28, 28),  # Adjust image size if needed\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',  # Use categorical for multi-class classification\n",
    "    color_mode='grayscale',  # Use grayscale if your images are grayscale\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b174c9a2-1cc3-4e64-98d9-6fbf6442add1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 8.3461 - accuracy: 0.5714 - val_loss: 3.9294 - val_accuracy: 0.8090\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9118 - accuracy: 0.8000 - val_loss: 0.0123 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0918 - accuracy: 0.9905 - val_loss: 0.1714 - val_accuracy: 0.9551\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0166 - accuracy: 0.9952 - val_loss: 0.0138 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.0140 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 6.6281e-04 - accuracy: 1.0000 - val_loss: 0.0530 - val_accuracy: 0.9775\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 6.5128e-04 - accuracy: 1.0000 - val_loss: 0.0312 - val_accuracy: 0.9775\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 2.5487e-04 - accuracy: 1.0000 - val_loss: 0.0180 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.2939e-04 - accuracy: 1.0000 - val_loss: 0.0256 - val_accuracy: 0.9775\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.1030e-04 - accuracy: 1.0000 - val_loss: 0.0287 - val_accuracy: 0.9775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1764e6310>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.fit(\n",
    "    train_generator,\n",
    "    epochs=10, \n",
    "    validation_data=test_generator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7634691-37d9-4a96-8a40-d448b8064568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0287 - accuracy: 0.9775\n",
      "Test accuracy: 0.9775280952453613\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = my_model.evaluate(test_generator)\n",
    "print(f\"Test accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6352bd48-e083-41cc-9fc8-6e0977b1fb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.save('kanji_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa834020-abaf-48bf-a3ce-45411b12431f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'体': 0, '日': 1}\n"
     ]
    }
   ],
   "source": [
    "print(train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048066dd-c861-4cdb-a8b5-36d6e6b91c43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
